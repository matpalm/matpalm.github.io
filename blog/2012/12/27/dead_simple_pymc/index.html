


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" 
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    
  <title>brain of mat kelcey</title>
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
<link rel="alternate" type="application/atom+xml" title="Atom 1.0"
href="/blog/feed/atom/index.xml" />
<link rel='stylesheet' href='/css/pygments_murphy.css' type='text/css' />
<link rel='stylesheet' href='/css/table.css' type='text/css' />
<link rel="stylesheet" href="/css/prettify.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/javascript" src="/js/prettify.js"></script>


  </head>
  <body onload="prettyPrint()">
    <div id="content">
      
  <div style="float:right">
<form action="http://www.google.com/search" method="get" class="roundtopsmall">
 <input value="matpalm.com" name="sitesearch" type="hidden">
 <input size="25" name="q" id="query" type="text" value="">&nbsp;
 <input name="Search" value="Search" type="submit">
</form>
</div>


<div style="float:right; padding-right:3em">
<a href="http://twitter.com/mat_kelcey"><img src="/blog/imgs/twit.png"></img></a>
<a href="https://github.com/matpalm"><img src="/blog/imgs/github.png"></img></a>
</div>

<h1><a href="/blog">brain of mat kelcey</a></h1>

<hr/>


      <div id="main_block">
        <div id="prose_block">
          

<div class="blog_post">
  <a name="dead-simple-pymc"></a>
  <h2 class="blog_post_title"><a href="/blog/2012/12/27/dead_simple_pymc" rel="bookmark" title="Permanent Link to dead simple pymc">dead simple pymc</a></h2>
  <small>December 27, 2012 at 09:00 PM | categories:

<a href='/blog/tag/uncategorized'>Uncategorized</a>
</small><p/>
  <div class="post_prose">
    
  <h3>pymc / mcmc / mind blown</h3>
<p><a href="http://pymc-devs.github.com/pymc/">PyMC</a>
   is a python library for working with
   <a href="http://en.wikipedia.org/wiki/Bayesian_statistics">bayesian statistical models</a>,
   primarily using
   <a href="http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a>
   methods. as a software engineer who has only just scratched the surface of statistics this whole MCMC
   business is <a href="/blog/imgs/jackie.jpg">blowing my mind</a> so i've got to share some examples.
</p>

<h3>fitting a normal</h3>
<p>let's start with the simplest thing possible, fitting a simple distribution.
</p>
<p>say we have a thousand values, <code> 87.27, 67.98, 119.56, ...</code> and we want to build a model of them.
</p>
<p>a common first step might be to generate a histogram.
</p>
<img src="/blog/imgs/2012/pymc/data.eg2.png" />

<p>if i had to a make a guess i'd say this data looks
   <a href="http://en.wikipedia.org/wiki/Normal_distribution">normally distributed</a>.
   somewhat unsurprising, not just because normal distributions are freakin everywhere,
   (this great
   <a href="http://www.youtube.com/watch?v=JNm3M9cqWyc">khan academy video</a>
   on
   <a href="http://en.wikipedia.org/wiki/Central_limit_theorem">the central limit theorem</a>
   explains why) but because it was me who synthetically generated this data in the first place ;)
</p>
<p>now a normal distribution is parameterised by two values; it's <i>mean</i> (technically speaking, the "middle" of the curve) and it's <i>standard deviation</i> (even more technically speaking, how "fat" it is) so let's use PyMC to figure out what these values are for this data.
</p>
<p><i>!!warning!! !!!total overkill alert!!!</i> there must be a bazillion simpler ways to fit a normal to this data but this post is about
   dead-simple-PyMC not dead-simple-something-else.
</p>
<p>first a definition of our model.
</p>
<pre class="prettyprint linenums">
# simple_normal_model.py
from pymc import *
data = map(float, open('data', 'r').readlines())
mean = Uniform('mean', lower=min(data), upper=max(data))
precision = Uniform('precision', lower=0.0001, upper=1.0)
process = Normal('process', mu=mean, tau=precision, value=data, observed=True)
</pre>

<p>working <i>backwards</i> through this code ...
</p>
<ul>
 <li>
     line 6 says i am trying to model some <code>process</code> that i believe is <code>Normal</code>ly distributed defined by variables <code>mean</code> and <code>precision</code>. (precision is just the inverse of the variance, which in turn is just the standard deviation squared). i've already <code>observed</code> this data and the <code>value</code>s are in the variable <code>data</code>
 </li>

 <li>
     line 5 says i don't know the <code>precision</code> for my <code>process</code> but my prior belief is it's value is somewhere between 0.0001 and 1.0.
since i don't favor any values in this range my belief is <code>uniform</code> across the values. note: assuming a uniform distribution for the precision is overly simplifying things quite a bit, but we can get away with it in this simple example and we'll come back to it.
 </li>

 <li>
     line 4 says i don't know the <code>mean</code> for my data but i think it's somewhere between the <code>min</code> and the <code>max</code> of the observed <code>data</code>. again this belief is <code>uniform</code> across the range.
 </li>

 <li>
     line 3 says the <code>data</code> for my unknown <code>process</code> comes from a local file (just-plain-python)
 </li>
</ul>
<p>the second part of the code runs the MCMC sampling.
</p>
<pre class="prettyprint linenums">
from pymc import *
import simple_normal_model
model = MCMC(simple_normal_model)
model.sample(iter=500)
print(model.stats())
</pre>

<p>working <i>forwards</i> through this code ...
</p>
<ul>
 <li>
     line 4 says build a MCMC for the model from the <code>simple_normal_model</code> file
 </li>

 <li>
     line 5 says run a sample for 500 iterations
 </li>

 <li>
     line 6 says print some stats.
 </li>
</ul>
<p><b>and that's it!</b>
</p>
<p>the output from our stats includes among other things estimates for the <code>mean</code> and <code>precision</code> we were trying to find
</p>
<pre class="prettyprint">
{
'mean': {'95% HPD interval': array([  94.53688316,  102.53626478]) ... },
'precision': {'95% HPD interval': array([ 0.00072487,  0.03671603]) ... },
...
}
</pre>

<p>now i've brushed over a couple of things here
   (eg the use of uniform prior over the precision, see <a href="http://pymc-devs.github.com/pymc/modelfitting.html#gibbs-step-methods">here</a> for more details)
   but i can get away with it all because this problem is a trivial one and i'm not doing gibbs sampling in this case.
   the main point i'm trying to make is that it's dead simple to start writing these models.
</p>
<p>one thing i do want to point out is that this estimation doesn't result in just one single value for mean and precision, it results in a distribution of
   the possible values. this is great since it gives us an idea of how confident we can be in the values as well as allowing this whole process to be iterative,
   ie the output values from this model can be feed easily into another.
</p>

<h3>deterministic variables</h3>
<p>all the code above parameterised the normal distribution with a mean and a precision. i've always thought of normals though in terms of means and standard deviations
   (precision is a more bayesian way to think of things... apparently...) so the first extension to my above example i want to make is to redefine the problem
   in terms of a prior on the standard deviation instead of the precision. mainly i want to do this to introduce the <code>deterministic</code> concept
   but it's also a subtle change in how the sampling search will be directed because it introduces a non linear transform.
</p>
<pre class="prettyprint linenums">
data = map(float, open('data', 'r').readlines())

mean = Uniform('mean', lower=min(data), upper=max(data))
std_dev = Uniform('std_dev', lower=0, upper=50)

@deterministic(plot=False)
def precision(std_dev=std_dev):
    return 1.0 / (std_dev * std_dev)

process = Normal('process', mu=mean, tau=precision, value=data, observed=True)
</pre>

<p>our code is almost the same but instead of a prior on the <code>precision</code> we use a <code>deterministic</code> method to map from the parameter we're
   trying to fit (the <code>precision</code>) to a variable we're trying to estimate (the <code>std_dev</code>).
</p>
<p>we fit the model using the same <code>run_mcmc.py</code> but this time get estimates for the <code>std_dev</code> not the <code>precision</code>
</p>
<pre class="prettyprint">
{
'mean': {'95% HPD interval': array([  94.23147867,  101.76893808]), ...
'std_dev': {'95% HPD interval': array([ 19.53993697,  21.1560098 ]), ...
...
}
</pre>

<p>which all matches up to how i originally generated the data in the first place.. cool!
</p>
<pre class="prettyprint">
from numpy.random import normal
data = [normal(100, 20) for _i in xrange(1000)]
</pre>

<p>for this example let's now dive a bit deeper than just the stats object.
   to help understand how the sampler is converging on it's results we can also dump
   a trace of it's progress at the end of <code>run_mcmc.py</code>
</p>
<pre class="prettyprint">
import numpy
for p in ['mean', 'std_dev']:
    numpy.savetxt("%s.trace" % p, model.trace(p)[:])
</pre>

<p>plotting this we can see how quickly the sampled values converged.
</p>
<img src="/blog/imgs/2012/pymc/traces.eg2.png" />


<h3>two normals</h3>
<p>let's consider a slightly more complex example.
</p>
<p>again we have some data... <code>107.63, 207.43, 215.84, ...</code> that plotted looks like this...
</p>
<img src="/blog/imgs/2012/pymc/data.eg3.png" />

<p>hmmm. looks like <i>two</i> normals this time with the one centered on 100 having a bit more data.
</p>
<p>how could we model this one?
</p>
<pre class="prettyprint linenums">
data = map(float, open('data', 'r').readlines())

theta = Uniform("theta", lower=0, upper=1)
bern = Bernoulli("bern", p=theta, size=len(data))

mean1 = Uniform('mean1', lower=min(data), upper=max(data))
mean2 = Uniform('mean2', lower=min(data), upper=max(data))
std_dev = Uniform('std_dev', lower=0, upper=50)

@deterministic(plot=False)
def mean(bern=bern, mean1=mean1, mean2=mean2):
    return bern * mean1 + (1 - ber) * mean2

@deterministic(plot=False)
def precision(std_dev=std_dev):
    return 1.0 / (std_dev * std_dev)

process = Normal('process', mu=mean, tau=precision, value=data, observed=True)
</pre>

<p>reviewing the code again it's mostly the same the big difference being the <code>deterministic</code> definition of the <code>mean</code>.
   it's now that we finally start to show off the awesome power of these non analytical approaches.
</p>
<p>line 12 defines the mean not by one <code>mean</code> variable
   but instead as a mixture of two, <code>mean1</code> and <code>mean2</code>. for each value we're trying to model we pick either <code>mean1</code>
   or <code>mean2</code> based on <i>another</i> random variable <code>bern</code>.
   <code>bern</code> is described by a
   <a href="http://en.wikipedia.org/wiki/Bernoulli_distribution">bernoulli distribution</a>
   and so is either 1 or 0, proportional to the parameter <code>theta</code>.
</p>
<p>ie the definition of our <code>mean</code> is that when <code>theta</code> is high, near 1.0, we pick <code>mean1</code> most of the time and
   when <code>theta</code> is low, near 0.0, we pick <code>mean2</code> most of the time.
</p>
<p>what we are solving for then is not just <code>mean1</code> and <code>mean2</code> but how the values are split between them (described by <code>theta</code>)
   (and note for the sake of simplicity i made the two normal differ in their means but use a shared standard deviation. depending on what you're doing this
   might or might not make sense)
</p>
<p>reviewing the traces we can see the converged <code>mean</code>s are 100 &amp; 200 with <code>std dev</code> 20. the mix (<code>theta</code>) is 0.33, which all agrees
   with the synthetic data i generated for this example...
</p>
<p><img src="/blog/imgs/2012/pymc/traces.eg3.png" /><img src="/blog/imgs/2012/pymc/trace.theta.png" />
</p>
<pre class="prettyprint">
from numpy.random import normal
import random
data = [normal(100, 20) for _i in xrange(1000)]  # 2/3rds of the data
data += [normal(200, 20) for _i in xrange(500)]  # 1/3rd of the data
random.shuffle(data)
</pre>

<p>to me the awesome power of these methods is the ability in that function to pretty much write whatever i think best describes the process. too cool for school.
</p>
<p>i also find it interesting to see how the convergence came along...
   the model starts in a local minima of both normals having mean a bit below 150
   (the midpoint of the two actual ones) with a mixing proportion of somewhere in the ballpark of 0.5 / 0.5.
   around iteration 1500 it correctly splits them apart and starts to understand the mix is more like 0.3 / 0.7.
   finally by about iteration 2,500 it starts working on the standard deviation which in turn really helps narrow down the true means.
</p>
<p>(thanks <a href="https://twitter.com/Cmrn_DP">cam</a> for helping me out with the formulation of this one..)
</p>

<h3>summary and further reading</h3>
<p>these are pretty simple examples thrown together to help me learn but i think they're still illustrative of the power of these methods
   (even when i'm completely ignore anything
   to do with
   <a href="http://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">conjugacy</a>)
</p>
<p>in general i've been working through an awesome book,
   <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">doing bayesian data analysis</a>,
   and can't recommend it enough.
</p>
<p>i also found <a href="https://twitter.com/johnmyleswhite">john's</a>
   blog post on
   <a href="http://www.johnmyleswhite.com/notebook/2010/08/20/using-jags-in-r-with-the-rjags-package/">using jags in r</a>
   was really helpful getting me going.
</p>
<p>all the examples listed here are on
   <a href="https://github.com/matpalm/doing_bayesian_data_analysis/tree/master/pymc_hacking">github</a>.
</p>
<p>next is to rewrite everything in <a href="http://mc-stan.org/">stan</a> and do some comparision between pymc, stan and
   <a href="http://mcmc-jags.sourceforge.net/">jags</a>. fun times!
</p>

  </div>
</div>



<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_url = "http://matpalm.com/blog/2012/12/27/dead_simple_pymc";
</script>

        </div><!-- End Prose Block -->
      </div><!-- End Main Block -->
      <div id="footer" style="float:right;">
        
  <hr/>
  <p id="credits">
Powered by <a href="http://www.blogofile.com">Blogofile</a>.<br/>
<br/>
RSS feeds for <a href="/blog/feed/index.xml">Entries</a>
<br>
</p>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-10409220-1");
pageTracker._trackPageview();
} catch(err) {}</script>



      </div> <!-- End Footer -->
    </div> <!-- End Content -->
  </body>
</html>




