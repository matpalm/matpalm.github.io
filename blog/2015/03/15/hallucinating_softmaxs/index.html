


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" 
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    
  <title>brain of mat kelcey</title>
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
<link rel="alternate" type="application/atom+xml" title="Atom 1.0"
href="/blog/feed/atom/index.xml" />
<link rel='stylesheet' href='/css/pygments_murphy.css' type='text/css' />
<link rel='stylesheet' href='/css/table.css' type='text/css' />
<link rel="stylesheet" href="/css/prettify.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/javascript" src="/js/prettify.js"></script>


  </head>
  <body onload="prettyPrint()">
    <div id="content">
      
  <div style="float:right">
<form action="http://www.google.com/search" method="get" class="roundtopsmall">
 <input value="matpalm.com" name="sitesearch" type="hidden">
 <input size="25" name="q" id="query" type="text" value="">&nbsp;
 <input name="Search" value="Search" type="submit">
</form>
</div>


<div style="float:right; padding-right:3em">
<a href="http://twitter.com/mat_kelcey"><img src="/blog/imgs/twit.png"></img></a>
<a href="https://github.com/matpalm"><img src="/blog/imgs/github.png"></img></a>
</div>

<h1><a href="/blog">brain of mat kelcey</a></h1>

<hr/>


      <div id="main_block">
        <div id="prose_block">
          

<div class="blog_post">
  <a name="hallucinating-softmaxs"></a>
  <h2 class="blog_post_title"><a href="/blog/2015/03/15/hallucinating_softmaxs" rel="bookmark" title="Permanent Link to hallucinating softmaxs">hallucinating softmaxs</a></h2>
  <small>March 15, 2015 at 10:00 PM | categories:

<a href='/blog/tag/uncategorized'>Uncategorized</a>
</small><p/>
  <div class="post_prose">
    
  <h2>language models</h2>
<p>language modelling is a classic problem in NLP; given a sequence of words such as "my cat likes to ..." what's the next word? this 
   problem is related to all sorts of things, everything from autocomplete to speech to text.
</p>
<p>the classic solution to language modelling is based on just counting. if a speech to text system is sure its heard "my cat likes to" but
   then can't decide if the next word if "sleep" or "beep" we can just look at relative counts; if we've observed in a large corpus
   how cats like to sleep more than they like to beep we can say "sleep" is more likely. (note: this would be different
   if it was "my roomba likes to ...")
</p>
<p>the first approach i saw to solving this problem with neural nets is from bengio et al. 
   <a href="http://jmlr.csail.mit.edu/papers/volume3/bengio03a/bengio03a.pdf">"a neural probabilistic language model"</a> (2003).
   this paper was a huge eye opener for me and was the first case i'd seen of using a distributed, rather than purely symbolic, 
   representation of text. definitely "word embeddings" are all the rage these days!
</p>
<p>bengio takes the approach of using a softmax to estimate the distribution of possible words given the two previous words.
   ie \( P({w}_3 | {w}_1, {w}_2) \). depending on your task though it might make more sense to instead estimate the
   likelihood of the triple directly ie \( P({w}_1, {w}_2, {w}_3) \). 
</p>
<p>let's work through a empirical comparison of these two on a synthetic problem. we'll call the first the <i>softmax</i> approach and the second the <i>logisitic_regression</i> approach.
</p>

<h2>a simple generating grammar.</h2>
<p>rather than use real text data let's work on a simpler synthetic dataset with a vocab of only 6 tokens; "A", "B", "C", "D", "E" &amp; "F". 
   be warned: a vocab this small is so contrived that it's hard to generalise any result from it. in particular a normal english vocab in
   the hundreds of thousands would be soooooooo much sparser.
</p>
<p>we'll use random walks on the following erdos renyi graph as a generating grammar. egs "phrases" include
   "D C", "A F A F A A", "A A", "E D C" &amp; "E A A"
</p>
<img width="800" src="/blog/imgs/2015/hsm/trigram_generating_graph.png"/>

<p>the main benefit of such a contrived small vocab is that it's feasible to analyse all 6<sup>3</sup> = 216 trigrams. 
   let's consider the distributions associated with a couple of specific (w1, w2) pairs.
</p>

<h3>F -&gt; A -&gt; ?</h3>
<p>there are only 45 trigrams that this grammar generates and the most frequent one is FAA. FAF is also possible but the other 
   FA? cases can never occur.
</p>
<pre>
F A A  0.20   # the most frequent trigram generated
F A B  0.0    # never generated
F A C  0.0
F A D  0.0
F A E  0.0
F A F  0.14   # the 4th most frequent trigram
</pre>


<h4>softmax</h4>
<p>if we train a simple softmax based neural probabilistic language model (nplm) we see the distribution
   of \( P({w}_3 | {w}_1=F, {w}_2=A ) \) converge to what we expect; FAA has a likelihood of 0.66, FAF has 0.33 and the others 0.0
</p>
<img src="/blog/imgs/2015/hsm/sm_FA.png" />

<p>this is a good illustration of the convergence we expect to see with a softmax. 
   each observed positive example of FAA is also an implicit negative example for FAB, FAC, FAD, FAE &amp; FAF
   and as such each FAA causes the likelihood of FAA to go up while pushing the others down. 
   since we observe FAA twice as much as FAF it gets twice the likelihood 
   and since we never see FAB, FAC, FAD or FAE they only ever get pushed down and converge to 0.0
</p>
<p>since the implementation behind this is (overly) simple we can run a couple of times to ensure things are converging consistently. 
   here's 6 runs, from random starting parameters, and we can see each converges to the same result..
</p>
<img src="/blog/imgs/2015/hsm/i_sm_FA.png"/>


<h4>logistic regression</h4>
<p>now consider the logisitic model where instead of learning the distribution of w3 given (w1, w2) we instead
   model the likelihood of the triple directly \( P({w}_1, {w}_2, {w}_3) \). 
   in this case we're modelling whether a specific example is true or not, not how it relates to others, 
   so one big con is that there are no implicit negatives like the softmax. we need explicit negative examples and for this 
   experiment i've generated them by random sampling the trigrams that don't occur in the observed set.
   ( the generation of "good" negatives is a surprisingly hard problem )
</p>
<p>if we do 6 runs again instead of learning the distribution we have FAA and FAF converging to 1.0 and the others converge to 0.0. 
   run4 actually has FAB tending to 1.0 too but i wouldn't be surprised at all if it dropped later; these graphs in general are what i'd
   expect given i'm just using a fixed global learning rate (ie nothing at all special about adapative learning rate)
</p>
<img src="/blog/imgs/2015/hsm/i_lr_FA.png"/>


<h3>C -&gt; B -&gt; ?</h3>
<p>now insteading considering the most frequent w1, w2 trigrams let's consider the least frequent.
</p>
<pre>
C B A  0.003
C B B  0.07   # 28th most frequent (of 45 possible trigrams)
C B C  0.0
C B D  0.003
C B E  0.002
C B F  0.001  # the least frequent trigram generated
</pre>


<h4>softmax</h4>
<p>as before the softmax learns the distribution; CBB is the most frequent, CBC has 0.0 probability and the others are roughly equal.
   these examples are far less frequent in the dataset so the model, quite rightly, allocates less of the models complexity to getting
   them right.
</p>
<img src="/blog/imgs/2015/hsm/i_sm_CB.png"/>


<h4>logisitic regression</h4>
<p>the logisitic model as before has, generally, everything converging to 1.0 except CBC which converges to 0.0
</p>
<img src="/blog/imgs/2015/hsm/i_lr_CB.png"/>


<h3>C -&gt; C -&gt; ?</h3>
<p>finally consider the case of C -&gt; C -&gt; ?. this one is interesting since C -&gt; C never actually occurs in the grammar.
</p>

<h4>logisitic regression</h4>
<p>first let's consider the logistic case. CC only ever occurs in the training data as an explicit negative so we see all of
   them converging to 0.0 ( amusingly in run4 CCC alllllmost made it )
</p>
<img src="/blog/imgs/2015/hsm/i_lr_CC.png"/>


<h4>softmax</h4>
<p>now consider the softmax. recall that the softmax learns by explicit positives and implicit negatives, but, since there are no
   cases of observed CC?, the softmax would not have seen any CC? cases.
</p>
<img src="/blog/imgs/2015/hsm/i_sm_CC.png"/>

<p>so what is going on here? the convergence is all over the place!
   run2 and run6 seems to suggest CCA is the only likely case whereas run3 and run4 oscillate between CCB and CCF ???
</p>
<p>it turns out these are artifacts of the training. there was no pressure in any way to get CC? "right" so these are just
   the side effects of how the embeddings for tokens, particularly C in this case, are being used for the other actual observed 
   examples. we call these hallucinations.
</p>
<p>another slightly different way to view this is to run the experiment 100 times and just consider the converged state
   (or at least the final state after a fixed number of iterations)
</p>
<p><img src="/blog/imgs/2015/hsm/final100.fa.png"/>
   <img src="/blog/imgs/2015/hsm/final100.cb.png"/>
   <img src="/blog/imgs/2015/hsm/final100.cc.png"/>
</p>
<p>if we consider FA again we can see its low variance convergence of FAA to 0.66 and FAF to 0.33. 
</p>
<p>if we consider CB again we can see its higher variance convergence to the numbers we reviewe before; CBB ~= 0.4, CBC = 0.0 and the
   others around 0.15
</p>
<p>considering CC though we see CCA and CCB have a bimodal distribution between 0.0 and 1.0 unlike any of the others. fascinating!
</p>

<h3>conclusions</h3>
<p>this is interesting but i'm unsure how much of it is just due to an overly simple model. this implementation just uses a simple
   fixed global learning rate (no per weight adaptation at all), uses very simple weight initialisation and has no regularisation at all :/
</p>
<p>all the code can be found on <a href="https://github.com/matpalm/neural_prob_lang_model/">github</a>
</p>

  </div>
</div>



<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_url = "http://matpalm.com/blog/2015/03/15/hallucinating_softmaxs";
</script>

        </div><!-- End Prose Block -->
      </div><!-- End Main Block -->
      <div id="footer" style="float:right;">
        
  <hr/>
  <p id="credits">
Powered by <a href="http://www.blogofile.com">Blogofile</a>.<br/>
<br/>
RSS feeds for <a href="/blog/feed/index.xml">Entries</a>
<br>
</p>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-10409220-1");
pageTracker._trackPageview();
} catch(err) {}</script>



      </div> <!-- End Footer -->
    </div> <!-- End Content -->
  </body>
</html>




