


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" 
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    
  <title>brain of mat kelcey...</title>
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
<link rel="alternate" type="application/atom+xml" title="Atom 1.0"
href="/blog/feed/atom/index.xml" />
<link rel='stylesheet' href='/css/pygments_murphy.css' type='text/css' />
<link rel='stylesheet' href='/css/table.css' type='text/css' />
<link rel="stylesheet" href="/css/prettify.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/javascript" src="/js/prettify.js"></script>


  </head>
  <body onload="prettyPrint()">
    <div id="content">
      
  <div style="float:right">
<form action="http://www.google.com/search" method="get" class="roundtopsmall">
 <input value="matpalm.com" name="sitesearch" type="hidden">
 <input size="25" name="q" id="query" type="text" value="">&nbsp;
 <input name="Search" value="Search" type="submit">
</form>
</div>


<div style="float:right; padding-right:3em">
<a href="http://twitter.com/mat_kelcey"><img src="/blog/imgs/twit.png"></img></a>
<a href="https://github.com/matpalm"><img src="/blog/imgs/github.png"></img></a>
</div>

<h1><a href="/blog">brain of mat kelcey...</a></h1>

<hr/>


      <div id="main_block">
        <div id="prose_block">
          

<div class="blog_post">
  <a name="a-jax-random-embedding-ensemble-network"></a>
  <h2 class="blog_post_title"><a href="/blog/jax_random_embedding_ensemble_network" rel="bookmark" title="Permanent Link to a jax random embedding ensemble network">a jax random embedding ensemble network</a></h2>
  <small>June 15, 2020 at 06:30 AM | categories:

<a href='/blog/tag/ensemble_nets'>ensemble_nets</a>, <a href='/blog/tag/jax'>jax</a>
</small><p/>
  <div class="post_prose">
    
  <h1>tl;dr</h1>
<p>random embedding networks can be used to generate weakly labelled data for metric learning and
   they see a large benefit from being run in ensembles.
</p>
<p>can we represent these ensembles as a single forward pass in jax?
   why yes! yes we can!
</p>

<h1>overview</h1>
<p>a fundamental question when training embedding models is deciding how to specify
   the examples we want to be close vs examples we want to be far apart.
</p>
<p>consider a collection of renders of random synthetic objects that look something like...
</p>
<img src="/blog/imgs/2020/jreen/5x5_sample.png" />

<p><i>( check out my
   <a href="https://github.com/matpalm/minimal_pybullet_on_dataflow_eg">minimal example of running pybullet under google cloud dataflow</a>
   if you'd like to generate a large amount of your own )</i>
</p>
<p>can we train an model that learns embeddings for these objects without explicit labels?
   i.e. is there a way of weakly labelling these?
   yes! by using random embedding networks.
</p>
<p>it turns out if we just initialise (don't train!) an embedding network
   that images of the same object will <em>sometimes</em> be embedded
   closest to each other.
   we can use this as a form of weak labelling; it just has to occur more often than random.
</p>
<p>consider a collage of 3 "test examples" with each example consisting of 11 images.
</p>
<ul>
 <li>
     first column: an <b>anchor</b> image which is a random render of a random object.
 </li>

 <li>
     second column: a <b>positive</b> image which is another render of the same object.
 </li>

 <li>
     9 remaining column: <b>negative</b> images which are renders of other objects.
 </li>
</ul>
<p>though this collage shows 3 examples the full test set we'll be using will contain <code>N=100</code> examples
</p>
<img src="/blog/imgs/2020/jreen/3_test_eg_sample.png" />

<p>we can quantify the quality of a random embedding network by embedding all 11 of these
   images and seeing how often the anchor is closer to the positive than any of the negatives.
   given there are 9 negatives for each positive we know that random choice accuracy would
   be 0.1.
   the question is then, can we do better with random embeddings?
</p>

<h1>code</h1>
<p><i>to repro all this have a look at this far less commented
   <a href="https://colab.research.google.com/drive/1FOWuvaWjiq7_PW_Lno09aUMHDPMfoYyz?usp=sharing">colab</a></i>
</p>
<p>first let's get some pre cooked test data ...
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; !wget "https://github.com/matpalm/shared_data/blob/master/test_set_array.npy.gz?raw=true" -O test_set_array.npy.gz --quiet
&gt;&gt;&gt; !gunzip -f test_set_array.npy.gz
&gt;&gt;&gt;
&gt;&gt;&gt; test_set_array = np.load("test_set_array.npy")
&gt;&gt;&gt; N = test_set_array.shape[0]  # number of examples in test set
</code></pre>

<p>this array is shaped <code>(N, 11, HW, HW, 3)</code>; and again, as before
</p>
<ul>
 <li>
     <code>N = 100</code> represents the 100 test set examples.
 </li>

 <li>
     11 represents the anchor, positive &amp; 9 negatives.
 </li>

 <li>
     <code>(HW, HW, 3)</code> represents the image tensor
 </li>
</ul>

<h2>a random embedding keras model</h2>
<p>let's start with a basic <a href="https://keras.io/">keras</a> model.
   this model is taken from another one of my projects and forms the basis of a fully convolutional network.
   in this form it is structured to have a convolutional stack that takes an input of <code>(32, 32, 3)</code> and outputs a <code>(1, 1, 256)</code> spatial feature map.
   this feature map is passed through a Dense layer with ReLU activation (implemented as a 1x1 convolution)
   followed by a linear projection to a <code>E</code> dimensional embedding.
   we normalise the embeddings to unit length for ease of the upcoming similarity calculations.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; E = 32   # output embedding dimension used for all models
&gt;&gt;&gt;
&gt;&gt;&gt; class NormaliseLayer(Layer):
&gt;&gt;&gt;     def call(self, x):
&gt;&gt;&gt;         return tf.nn.l2_normalize(x, axis=-1)
&gt;&gt;&gt;
&gt;&gt;&gt; def conv(x, filters):
&gt;&gt;&gt;     return Conv2D(filters=filters, kernel_size=3, strides=2,
&gt;&gt;&gt;                   padding='VALID', kernel_initializer='orthogonal',
&gt;&gt;&gt;                   activation='relu')(x)
&gt;&gt;&gt;
&gt;&gt;&gt; def construct_model():
&gt;&gt;&gt;     inputs = Input(shape=(HW, HW, 3))
&gt;&gt;&gt;     model = conv(inputs, 32)
&gt;&gt;&gt;     model = conv(model, 64)
&gt;&gt;&gt;     model = conv(model, 128)
&gt;&gt;&gt;     model = conv(model, 256)
&gt;&gt;&gt;     model = Dense(units=32, kernel_initializer='orthogonal',
&gt;&gt;&gt;                   activation='relu')(model)
&gt;&gt;&gt;     embeddings = Dense(units=E, kernel_initializer='orthogonal',
&gt;&gt;&gt;                       activation=None, name='embedding')(model)
&gt;&gt;&gt;     embeddings = NormaliseLayer()(embeddings)
&gt;&gt;&gt;     return Model(inputs, embeddings)
&gt;&gt;&gt;
&gt;&gt;&gt; model = construct_model()
&gt;&gt;&gt; model.summary()

</code></pre>

<pre class="prettyprint">
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #
    =================================================================
    input_1 (InputLayer)         [(None, 32, 32, 3)]       0
    conv2d (Conv2D)              (None, 15, 15, 32)        896
    conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496
    conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856
    conv2d_3 (Conv2D)            (None, 1, 1, 256)         295168
    dense (Dense)                (None, 1, 1, 32)          8224
    embedding (Dense)            (None, 1, 1, 32)          1056
    normalise_layer (NormaliseLa (None, 1, 1, 32)          0
    =================================================================
    Total params: 397,696
    Trainable params: 397,696
    Non-trainable params: 0
</pre>

<p>we'll need some utility functions to
</p>
<ol>
 <li>
     run images through this model...
 </li>

 <li>
     calculate the cosine sims between the different types of the embeddings and
 </li>

 <li>
     calculate an overall accuracy for the 100 test examples.
 </li>
</ol>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; def embeddings_for(model, imgs):
&gt;&gt;&gt;     embeddings = model.predict(imgs.reshape(N*11, HW, HW, 3))
&gt;&gt;&gt;     return embeddings.reshape(N, 11, E)
&gt;&gt;&gt;
&gt;&gt;&gt; def anc_pos_neg_sims(embeddings):
&gt;&gt;&gt;     # slice anchors, positives and negatives out of embeddings
&gt;&gt;&gt;     # and calculate pair wise sims using dot product similarity.
&gt;&gt;&gt;     # returns (N, 10) representing N examples of anchor compared to
&gt;&gt;&gt;     # positives / negatives.
&gt;&gt;&gt;     anchor_embeddings = embeddings[:, 0]              # (N, E)
&gt;&gt;&gt;     positive_negative_embeddings = embeddings[:, 1:]  # (N, 10, E)
&gt;&gt;&gt;     return np.einsum('ne,npe->np', anchor_embeddings,
&gt;&gt;&gt;                      positive_negative_embeddings)    # (N, 10)
&gt;&gt;&gt;
&gt;&gt;&gt; def accuracy(sims):
&gt;&gt;&gt;     # given a set of (N, 10) sims calculate accuracy. note that the data has
&gt;&gt;&gt;     # been prepared such that correct element is the 0th element so accuracy
&gt;&gt;&gt;     # is simply the times the most similar is the 0th.
&gt;&gt;&gt;     most_similar = np.argmax(sims, axis=1)
&gt;&gt;&gt;     times_correct = sum(most_similar == 0)
&gt;&gt;&gt;     return times_correct / N
&gt;&gt;&gt;
&gt;&gt;&gt; accuracy(anc_pos_neg_sims(embeddings_for(model, test_set_array)))

0.26
</code></pre>

<p><i>( note: if you're new to <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">einsum</a>
   check out my <a href="https://www.youtube.com/watch?v=SOaYrnQtd9g">illustrative einsum example</a> explainer video where i walk through what
   the einsum operations in this post are doing )</i>
</p>
<p>great! this model does (slightly) better than random (which would have been 0.1). hooray!
</p>
<p>since this model isn't even trained, we should expect a degree of variance across a number of differently initialised models.
   let's build <code>M=10</code> models and see how things vary.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; M = 10  # number of models to run in ensemble
&gt;&gt;&gt;
&gt;&gt;&gt; models = [construct_model() for _ in range(M)]
&gt;&gt;&gt; per_model_sims = np.stack([anc_pos_neg_sims(embeddings_for(m, test_set_array)) for m in models])  # (M, N, 10)
&gt;&gt;&gt; accuracies = [accuracy(sims) for sims in per_model_sims]
&gt;&gt;&gt;
&gt;&gt;&gt; print("accuracies", accuracies)
&gt;&gt;&gt; print("mean", np.mean(accuracies), "std", np.std(accuracies))

accuracies [0.2, 0.29, 0.26, 0.29, 0.29, 0.26, 0.29, 0.26, 0.27, 0.32]
mean 0.273 std 0.03034798181098703
</code></pre>


<h2>an ensemble?</h2>
<p>given this amount of variance we can ask, how would an ensemble do?
   a great thing about the way this model is structured is that we can do a weighted ensemble in a really simple way.
</p>
<p>note that the prediction is based on the argmax of these similarities,
   this means to combine them in an ensemble all we need to do is sum them before the argmax!
</p>
<p>since the embeddings are unit length the cosine similarity is being constrained to (-1, 1).
   so when we sum them what we're doing is taking a form of weighted vote. nice!
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; accuracy(np.sum(per_model_sims, axis=0))

0.4
</code></pre>

<p>great! the weighted ensemble does noticably better than any of the individual random models (which in turn did better than random choice)
</p>
<p>but it feels a bit clumsy to have to run these <code>M</code> models sequentially.
   could we model this directly in a pass of a <em>single</em> network?
</p>

<h2>what is input batching doing?</h2>
<p>let's think about what batching inputs does;
</p>
<p>the simplest form of a model takes an input,
   applies some layers parameterised in some way (by theta here)
   to calculate some hidden layer values
   and finally produces an output.
</p>
<p>\( I \rightarrow f^1(\theta^1) \rightarrow H \rightarrow f^2(\theta^2) \rightarrow O \)
</p>
<p>what we almost always do though is run a batch of <code>B</code> examples through at a time.
   batching inputs allows us to make the best use of hardware acceleration,
   as well as getting use some theoretical benfits regarding optimisation.
   since we're using the same model we have a single set of thetas and we produce a batch of <code>B</code> outputs.
</p>
<p>\( I_B \rightarrow f^1(\theta^1) \rightarrow H_B \rightarrow f^2(\theta^2) \rightarrow O_B \)
</p>
<p>but what we want to do now is to not just have a batch of <code>B</code> inputs, but also a batch of <code>M</code> models as well.
</p>
<p>\( I_B \rightarrow f^1(\theta^1_M) \rightarrow H_{B,M} \rightarrow f^2(\theta^2_M) \rightarrow O_{B,M} \)
</p>
<p>though the algebra is no different this idea of having multiple sets of theta for a layer
   isn't something that naturally fits in the standard frameworks. :(
   maybe this is something you <em>could</em> get going with keras but the couple of attempts i tried met heavy framework resistance :/
   perhaps this functionality of arbitrary mapping seems a good fit for
   <a href="https://jax.readthedocs.io/en/latest/#">jax's</a>
   <a href="https://github.com/google/jax#auto-vectorization-with-vmap">vmap</a>; but how would it work?
</p>

<h2>a random embedding jax model</h2>
<p>first let's rebuild this network in a minimal way with jax but without any layer framework.
   not using a layer framework means we'll need to maintain our own set of model parameters.
   note that we for this model we're not going to bother with bias terms, they would be zero by initialisation anyways
   (we're remember not training at all, just building and running these networks)
</p>
<p>note we also more explicitly refer to the dense layer here as using a 1x1 kernel convolution.
   this is actually equivalent to what's happening the above keras model where a <code>Dense</code> layer on a <code>(H, W, C)</code>
   input automagically does things as a <code>1x1</code> convolution.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; key = random.PRNGKey(0)
&gt;&gt;&gt; _key, *subkeys = random.split(key, 7)
&gt;&gt;&gt;
&gt;&gt;&gt; conv1_kernel = orthogonal()(subkeys[0], (3, 3, 3, 32))
&gt;&gt;&gt; conv2_kernel = orthogonal()(subkeys[1], (3, 3, 32, 64))
&gt;&gt;&gt; conv3_kernel = orthogonal()(subkeys[2], (3, 3, 64, 128))
&gt;&gt;&gt; conv4_kernel = orthogonal()(subkeys[3], (3, 3, 128, 256))
&gt;&gt;&gt; dense_kernel = orthogonal()(subkeys[4], (1, 1, 256, 32))
&gt;&gt;&gt; embedding_kernel = orthogonal()(subkeys[5], (1, 1, 32, 32))
&gt;&gt;&gt;
&gt;&gt;&gt; conv_dimension_numbers = lax.conv_dimension_numbers((1, HW, HW, 3),  # input shape prototype
&gt;&gt;&gt;                                                     (3, 3, 1, 1),    # 2d kernel shape prototype
&gt;&gt;&gt;                                                     ('NHWC',         # input
&gt;&gt;&gt;                                                      'HWIO',         # kernel
&gt;&gt;&gt;                                                      'NHWC'))        # output
&gt;&gt;&gt;
&gt;&gt;&gt; def conv_block(stride, with_relu, input, kernel):
&gt;&gt;&gt;     no_dilation = (1, 1)
&gt;&gt;&gt;     block = lax.conv_general_dilated(input, kernel, (stride, stride), 'VALID',
&gt;&gt;&gt;                                      no_dilation, no_dilation,
&gt;&gt;&gt;                                      conv_dimension_numbers)
&gt;&gt;&gt;     if with_relu:
&gt;&gt;&gt;         block = relu(block)
&gt;&gt;&gt;     return block
&gt;&gt;&gt;
&gt;&gt;&gt; @jit
&gt;&gt;&gt; def model(input):                                                       # (N, 32, 32, 3)
&gt;&gt;&gt;     conv1_output = conv_block(2, True, input, conv1_kernel)             # (N, 15, 15, 32)
&gt;&gt;&gt;     conv2_output = conv_block(2, True, conv1_output, conv2_kernel)      # (N, 7, 7, 64)
&gt;&gt;&gt;     conv3_output = conv_block(2, True, conv2_output, conv3_kernel)      # (N, 3, 3, 128)
&gt;&gt;&gt;     conv4_output = conv_block(2, True, conv3_output, conv4_kernel)      # (N, 1, 1, 256)
&gt;&gt;&gt;     dense1_output = conv_block(1, True, conv4_output, dense_kernel)     # (N, 1, 1, E)
&gt;&gt;&gt;     embeddings = conv_block(1, False, dense1_output, embedding_kernel)  # (N, 1, 1, E)
&gt;&gt;&gt;     embeddings /= jnp.linalg.norm(embeddings, axis=-1, keepdims=True)   # (N, 1, 1, E)
&gt;&gt;&gt;     return embeddings
</code></pre>

<p>in terms of utility functions we can reuse the above <code>anc_pos_neg_sims</code> and <code>accuracy</code> from before
   but we'll need to change the <code>embeddings_for</code> slightly based on how jax and keras models are called differently.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; def embeddings_for(imgs):
&gt;&gt;&gt;     embeddings = model(imgs.reshape(N*11, HW, HW, 3))
&gt;&gt;&gt;     return embeddings.reshape(N, 11, E)
&gt;&gt;&gt;
&gt;&gt;&gt; accuracy(anc_pos_neg_sims(embeddings_for(test_set_array)))

0.27
</code></pre>


<h2>a single network ensemble?</h2>
<p>ok, a single random model does about the same.
   that helps give us confidence there's no weird behaviour difference the keras and the jax model.
   next, how do we handle the ensemble idea in a single network?
</p>
<p>firstly we need to remake all the kernels but with a leading <code>M</code> dimension that represents the
   <code>M</code> models we want to run in parallel.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; key = random.PRNGKey(0)
&gt;&gt;&gt; _key, *subkeys = random.split(key, 7)
&gt;&gt;&gt;
&gt;&gt;&gt; conv1_kernels = orthogonal()(subkeys[0], (M, 3, 3, 3, 32))
&gt;&gt;&gt; conv2_kernels = orthogonal()(subkeys[1], (M, 3, 3, 32, 64))
&gt;&gt;&gt; conv3_kernels = orthogonal()(subkeys[2], (M, 3, 3, 64, 128))
&gt;&gt;&gt; conv4_kernels = orthogonal()(subkeys[3], (M, 3, 3, 128, 256))
&gt;&gt;&gt; dense_kernels = orthogonal()(subkeys[4], (M, 1, 1, 256, 32))
&gt;&gt;&gt; embedding_kernels = orthogonal()(subkeys[5], (M, 1, 1, 32, 32))
</code></pre>

<p>and then all we have to do is run the same model as before, but using <code>vmap</code> for the calls.
</p>
<p>note: that the very first call vmaps over <em>only</em> the <code>conv1</code> layer parameters; that's since the input
   is just batched with <code>B</code>. this results in <code>conv1_output_m</code> picking up the additional <code>M</code> leading dimension.
   subsequent vmap calls are over <em>both</em> the prior layer inputs, and the layer parameters.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; @jit
&gt;&gt;&gt; def vmap_model(input):                                                                    # (N, 32, 32, 3)
&gt;&gt;&gt;     conv1_output_m = vmap(partial(conv_block, 2, True, input))(conv1_kernels)             # (M, N, 7, 7, 64)
&gt;&gt;&gt;     conv2_output_m = vmap(partial(conv_block, 2, True))(conv1_output_m, conv2_kernels)    # (M, N, 15, 15, 32)
&gt;&gt;&gt;     conv3_output_m = vmap(partial(conv_block, 2, True))(conv2_output_m, conv3_kernels)    # (M, N, 1, 1, 256)
&gt;&gt;&gt;     conv4_output_m = vmap(partial(conv_block, 2, True))(conv3_output_m, conv4_kernels)    # (M, N, 1, 1, E)
&gt;&gt;&gt;     dense1_output_m = vmap(partial(conv_block, 1, True))(conv4_output_m, dense_kernels)   # (M, N, 1, 1, E)
&gt;&gt;&gt;     embeddings = vmap(partial(conv_block, 1, False))(dense1_output_m, embedding_kernels)  # (M, N, 1, 1, E)
&gt;&gt;&gt;     embeddings /= jnp.linalg.norm(embeddings, axis=-1, keepdims=True)
&gt;&gt;&gt;     return embeddings
</code></pre>

<p>for the final example of running this model we forgo the previous utility functions and run each step explicitly.
</p>
<p>first we run the images through the model. this produces a <code>(M, N, 11, E)</code> output instead of a <code>(N, 11, E)</code> output as before.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; embeddings = vmap_model(test_set_array.reshape(N*11, HW, HW, 3)).reshape(M, N, 11, E)
&gt;&gt;&gt; embeddings.shape

(10, 100, 11, 32)
</code></pre>

<p>next we slice out the anchors from the positives and negatives
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; anchor_embeddings = embeddings[:, :, 0]                         # (M, N, E)
&gt;&gt;&gt; positive_negative_embeddings = embeddings[:, :, 1:]             # (M, N, 10, E)
</code></pre>

<p>we can calculate the per model accuracy by doing the reducing and explicitly keeping <code>m</code> in the output.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; per_model_sims = jnp.einsum('mne,mnpe->mnp',
&gt;&gt;&gt;                             anchor_embeddings,
&gt;&gt;&gt;                             positive_negative_embeddings)        # (M, N, 10)
&gt;&gt;&gt;
&gt;&gt;&gt; most_similar = jnp.argmax(per_model_sims, axis=2)    # (10, N)
&gt;&gt;&gt; times_correct = jnp.sum(most_similar == 0, axis=1)   # (10,)
&gt;&gt;&gt; accuracies = times_correct / N                       # (10,)
&gt;&gt;&gt; accuracies

array([0.25, 0.29, 0.24, 0.23, 0.21, 0.25, 0.3 , 0.25, 0.27, 0.31])
</code></pre>

<p>cool. so each model has some degree of variance, as the keras models did.
</p>
<p>finally we can represent the ensemble by simply doing a further reduction over <code>m</code> in the einsum calculation.
</p>
<pre class="prettyprint"><code class="language-python">&gt;&gt;&gt; ensemble_sims = jnp.einsum('mne,mnpe->np',
&gt;&gt;&gt;                            anchor_embeddings,
&gt;&gt;&gt;                            positive_negative_embeddings)       # (N, 10)
&gt;&gt;&gt;
&gt;&gt;&gt; most_similar = jnp.argmax(ensemble_sims, axis=1)  # (N,)
&gt;&gt;&gt; times_correct = jnp.sum(most_similar == 0)        # (1,)
&gt;&gt;&gt; accuracy = times_correct / N                      # (1,)
&gt;&gt;&gt; accuracy

DeviceArray(0.4, dtype=float32)
</code></pre>

<p>all this required only one forward pass through a single model,
   and with the weighted voting idea of the ensemble, we only required an extra <code>m</code> reduction in einsum sims call.
   so elegant!
</p>

  </div>
</div>



<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_url = "http://matpalm.com/blog/jax_random_embedding_ensemble_network";
</script>

        </div><!-- End Prose Block -->
      </div><!-- End Main Block -->
      <div id="footer" style="float:right;">
        
  <hr/>
  <p id="credits">
Powered by <a href="http://www.blogofile.com">Blogofile</a>.<br/>
<br/>
RSS feeds for <a href="/blog/feed/index.xml">Entries</a>
<br>
</p>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-10409220-1");
pageTracker._trackPageview();
} catch(err) {}</script>



      </div> <!-- End Footer -->
    </div> <!-- End Content -->
  </body>
</html>




